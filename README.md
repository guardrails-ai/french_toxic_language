# French Toxic Language Validator

| Developed by | Guardrails AI |
| --- | --- |
| Date of development | Oct 2024 |
| Validator type | Format, Toxic Language Detection |
| Blog |  |
| License | Apache 2 |
| Input/Output | Output |

## Description

### Intended Use
This validator is designed to detect toxic language (specifically insults) in French text using a list of insults scraped from Wiktionary. It leverages the `MauvaiseLangue` module to ensure that outputs generated by models do not contain inappropriate or offensive language.

### Requirements

* Dependencies:
	- guardrails-ai>=0.4.0
	- MauvaiseLangue>=1.0.8


## Installation

To install the validator, you can use Guardrails Hub as follows:

```bash
$ guardrails hub install hub://guardrails/french_toxic_language
```

## Usage Examples

### Validating string output via Python

In this example, we apply the `FrenchToxicLanguage` validator to a string output generated by an LLM.

```python
# Import Guard and Validator
from guardrails.hub import FrenchToxicLanguage
from guardrails import Guard

# Setup Guard
guard = Guard().use(
    FrenchToxicLanguage
)

guard.validate("Bonjour, comment ça va ?")  # Validator passes
guard.validate("Espèce de idiot, imbécile !")  # Validator fails
```

### Validating JSON output via Python

Here, we apply the `FrenchToxicLanguage` validator to a string field of a JSON output generated by an LLM.

```python
# Import Guard and Validator
from pydantic import BaseModel, Field
from guardrails.hub import FrenchToxicLanguage
from guardrails import Guard

# Initialize Validator
val = FrenchToxicLanguage()

# Create Pydantic BaseModel
class Message(BaseModel):
    message_text: str = Field(validators=[val])

# Create a Guard to check for valid Pydantic output
guard = Guard.from_pydantic(output_class=Message)

# Run LLM output generating JSON through guard
guard.parse("""
{
    "message_text": "Espèce de crétin !"
}
""")
```

# API Reference

**`__init__(self, on_fail="noop")`**
<ul>
Initializes a new instance of the FrenchToxicLanguage validator.

**Parameters**
- **`on_fail`** *(Callable, optional)*: The policy to enact when a validator fails. It can be one of `reask`, `fix`, `filter`, `refrain`, `noop`, `exception`, or `fix_reask`. It may also be a function that handles failed validation.

</ul>
<br/>

**`validate(self, value, metadata) -> ValidationResult`**
<ul>
Validates the given `value` by checking for toxic language (insults) in French using the scraped data from Wiktionary.

**Parameters**
- **`value`** *(Any)*: The input value to validate, typically a string.
- **`metadata`** *(dict)*: A dictionary containing metadata required for validation (optional).

**Returns**
- **`ValidationResult`**: Returns a `PassResult` if no toxic language is detected, or a `FailResult` if toxic language is found.

Example metadata:

    | Key | Type | Description | Default |
    | --- | --- | --- | --- |
    | `key1` | String | Description of key1's role. | N/A |

</ul>

### Example:

```python
validator = FrenchToxicLanguage()
result = validator.validate("Espèce de idiot, imbécile !", {})
assert isinstance(result, FailResult)
```
